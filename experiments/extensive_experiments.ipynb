{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cb0634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d166fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.explanation_evaluation_calc import (\n",
    "    load_jsonl, \n",
    "    evaluate_all_argllm, \n",
    "    evaluate_all_cot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71b2587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': 'What happens to you if you eat watermelon seeds?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'Where did fortune cookies originate?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'Why do veins appear blue?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What is the spiciest part of a chili pepper?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'How long should you wait before filing a missing person report?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'Why do matadors wave red capes?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What percentage of the brain does a human typically use?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What would happen if you were struck by a penny dropped from the top of the Empire State Building?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What U.S. state produces the most peaches?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'Why do chameleons change colors?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'On what date was the Declaration of Independence officially signed?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What color is the sun when viewed from space?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'Who composed the tune of \"Twinkle, Twinkle, Little Star\"?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What happens if an earthworm is cut in half?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'Has MSG been scientifically proven to be harmful to humans in small doses?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What happens if you go outdoors in cold weather with wet hair?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'How long should you wait between eating a meal and going swimming?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What happens if you swallow gum?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n",
      "{'q': 'What are the benefits of eating an apple a day?', 'redundancy': 1.0, 'weak_relevance': 1.0, 'strong_relevance': 1.0, 'coherence': -1.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(project_root, \"results\", \"generation\", \"cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\")\n",
    "cot_evaluation_results = evaluate_all_cot(filepath=file_path)\n",
    "\n",
    "for result in cot_evaluation_results:\n",
    "    print(result, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
