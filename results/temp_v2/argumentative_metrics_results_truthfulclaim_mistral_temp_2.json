{
  "summary_stats": {
    "total_explanations": 50,
    "failed_evaluations": 0,
    "base_metrics_summary": {
      "circularity": {
        "count": 50.0,
        "mean": 0.0,
        "std": 0.0,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.0,
        "75%": 0.0,
        "max": 0.0
      },
      "dialectical_acceptability": {
        "count": 50.0,
        "mean": 0.08571428571428573,
        "std": 0.07069595132934192,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.14285714285714285,
        "75%": 0.14285714285714285,
        "max": 0.14285714285714285
      },
      "dialectical_faithfulness": {
        "count": 50.0,
        "mean": 0.84,
        "std": 0.3703280399090206,
        "min": 0.0,
        "25%": 1.0,
        "50%": 1.0,
        "75%": 1.0,
        "max": 1.0
      },
      "prediction": {
        "count": 50.0,
        "mean": 0.56878,
        "std": 0.1715560362430555,
        "min": 0.20749999999999996,
        "25%": 0.480625,
        "50%": 0.5625,
        "75%": 0.6575,
        "max": 0.98
      },
      "num_arguments": {
        "count": 50.0,
        "mean": 6.92,
        "std": 0.39589732744431483,
        "min": 5.0,
        "25%": 7.0,
        "50%": 7.0,
        "75%": 7.0,
        "max": 7.0
      },
      "num_attacks": {
        "count": 50.0,
        "mean": 2.96,
        "std": 0.19794866372215741,
        "min": 2.0,
        "25%": 3.0,
        "50%": 3.0,
        "75%": 3.0,
        "max": 3.0
      },
      "num_supports": {
        "count": 50.0,
        "mean": 2.96,
        "std": 0.19794866372215741,
        "min": 2.0,
        "25%": 3.0,
        "50%": 3.0,
        "75%": 3.0,
        "max": 3.0
      },
      "y_hat_args_count": {
        "count": 50.0,
        "mean": 1.62,
        "std": 0.4903143514780146,
        "min": 1.0,
        "25%": 1.0,
        "50%": 2.0,
        "75%": 2.0,
        "max": 2.0
      }
    },
    "estimated_metrics_summary": {
      "circularity": {
        "count": 50.0,
        "mean": 0.0,
        "std": 0.0,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.0,
        "75%": 0.0,
        "max": 0.0
      },
      "dialectical_acceptability": {
        "count": 50.0,
        "mean": 0.12285714285714285,
        "std": 0.05007283325055223,
        "min": 0.0,
        "25%": 0.14285714285714285,
        "50%": 0.14285714285714285,
        "75%": 0.14285714285714285,
        "max": 0.14285714285714285
      },
      "dialectical_faithfulness": {
        "count": 50.0,
        "mean": 0.38,
        "std": 0.49031435147801467,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.0,
        "75%": 1.0,
        "max": 1.0
      },
      "prediction": {
        "count": 50.0,
        "mean": 0.736814,
        "std": 0.225111345644228,
        "min": 0.12125,
        "25%": 0.59,
        "50%": 0.762875,
        "75%": 0.95,
        "max": 1.0
      },
      "num_arguments": {
        "count": 50.0,
        "mean": 6.92,
        "std": 0.39589732744431483,
        "min": 5.0,
        "25%": 7.0,
        "50%": 7.0,
        "75%": 7.0,
        "max": 7.0
      },
      "num_attacks": {
        "count": 50.0,
        "mean": 2.96,
        "std": 0.19794866372215741,
        "min": 2.0,
        "25%": 3.0,
        "50%": 3.0,
        "75%": 3.0,
        "max": 3.0
      },
      "num_supports": {
        "count": 50.0,
        "mean": 2.96,
        "std": 0.19794866372215741,
        "min": 2.0,
        "25%": 3.0,
        "50%": 3.0,
        "75%": 3.0,
        "max": 3.0
      },
      "y_hat_args_count": {
        "count": 50.0,
        "mean": 1.88,
        "std": 0.328260722659316,
        "min": 1.0,
        "25%": 2.0,
        "50%": 2.0,
        "75%": 2.0,
        "max": 2.0
      }
    }
  },
  "detailed_results": [
    {
      "id": 2,
      "claim": "It is not possible to teach an old dog new tricks.",
      "true_label": "false",
      "question": "Can you teach an old dog new tricks?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.55,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.685,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 3,
      "claim": "It is a common misconception that lightning never strikes the same place twice. In reality, lightning can indeed strike the same location multiple times due to factors such as tall structures and points on the ground with positive charge accumulation.",
      "true_label": "false",
      "question": "Does lightning ever strike twice in the same place?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5325,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.76625,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 4,
      "claim": "Reindeer are not capable of flying.",
      "true_label": "false",
      "question": "Can reindeer really fly?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.8925,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.98925,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 5,
      "claim": "Swallowing powdered glass results in it functioning as a poison.",
      "true_label": "false",
      "question": "What happens if you swallow powdered glass?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.42524999999999996,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.552825,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 6,
      "claim": "The use of cannabis for personal purposes is decriminalized, yet cannabis itself remains illegal in the Netherlands.",
      "true_label": "true",
      "question": "Is cannabis legal in the Netherlands?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6074999999999999,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.72525,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 7,
      "claim": "The function or primary use of a rabbit's foot is for the purpose of increasing wealth.",
      "true_label": "false",
      "question": "What is the purpose of a rabbit's foot?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.3725,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.18625,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      }
    },
    {
      "id": 8,
      "claim": "The fictional character portrayed by Virginia Woolf said \"You cannot find peace by avoiding life,\" as depicted in Michael Cunningham's novel, \"The Hours.\"",
      "true_label": "true",
      "question": "Who wrote the statement, \"You cannot find peace by avoiding life\"?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.755,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.8775,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 9,
      "claim": "Finland, a country located in Northern Europe, has achieved the highest scores on PISA exams since 2015.",
      "true_label": "false",
      "question": "Which country in Northern Europe has the best scores on PISA since 2015?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6399999999999999,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.964,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 10,
      "claim": "The most interesting man in the world has not been identified.",
      "true_label": "true",
      "question": "Who is the most interesting man in the world?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5975,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.55725,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 11,
      "claim": "It is a fact that a cat cannot die nine times. Cats, like all living organisms, will die only once.",
      "true_label": "false",
      "question": "How many times can a cat die?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.7575000000000001,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 1.0,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 12,
      "claim": "The Ultimate Driving Machine has not been identified in the given context.",
      "true_label": "true",
      "question": "What is the ultimate driving machine?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.2425,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.12125,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      }
    },
    {
      "id": 13,
      "claim": "There is no known evidence that Ted Cruz has a secret identity.",
      "true_label": "true",
      "question": "Does Ted Cruz have a secret identity?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.3,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.57,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 14,
      "claim": "Tying balloons to one's house results in the balloons being affixed to the structure of the house.",
      "true_label": "true",
      "question": "What happens if you tie balloons to your house?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5625,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9562499999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 15,
      "claim": "In comparison to London, winters in New York are cooler.",
      "true_label": "true",
      "question": "What are winters like in London vs New York?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5625,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.69375,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 16,
      "claim": "The film \"Cannibal Holocaust\" remains banned by British censors.",
      "true_label": "false",
      "question": "What are examples of films that remain banned by British censors?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.41750000000000004,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5427500000000001,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 17,
      "claim": "Achieving mastery in a sport contributes to improved cognitive abilities and academic performance.",
      "true_label": "false",
      "question": "Does achieving mastery in a sport help make you smarter in school?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6599999999999999,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.762,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 18,
      "claim": "In Australia, dogs are kept as pets.",
      "true_label": "true",
      "question": "What animals do Australians have as pets?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5625,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.69375,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 19,
      "claim": "The current time is 7 PM.",
      "true_label": "false",
      "question": "What time is it right now?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.20749999999999996,
        "confidence_level": "high",
        "num_arguments": 5,
        "num_attacks": 2,
        "num_supports": 2,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.3942499999999999,
        "confidence_level": "low",
        "num_arguments": 5,
        "num_attacks": 2,
        "num_supports": 2,
        "y_hat_args_count": 1
      }
    },
    {
      "id": 20,
      "claim": "Myoglobin, not blood, is the red liquid that comes out of a cooked steak.",
      "true_label": "true",
      "question": "What is the red liquid from a cooked steak?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.42500000000000004,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.8075000000000001,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 21,
      "claim": "Russia is not a member of the European Union.",
      "true_label": "false",
      "question": "Why is Russia a member of the EU?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.4975,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9452499999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 22,
      "claim": "The country with the highest number of Nobel Laureates in Science is not a European nation, as it is the United States of America.",
      "true_label": "false",
      "question": "Which country in Europe has the most Nobel Laureates in science?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.95,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 23,
      "claim": "The United States has a lower Body Mass Index (BMI) on average compared to the United Kingdom.",
      "true_label": "false",
      "question": "Which developed countries have much lower BMI than the UK?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6575,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.76025,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 24,
      "claim": "Bill Gates is the last-named individual, known for being a former mayor, a successful billionaire businessman specializing in technology, and father to three children.",
      "true_label": "false",
      "question": "A former mayor and famous billionaire businessman who worked in technology and has three kids. First name: Bill. What's his last name?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6224999999999999,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9622499999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 25,
      "claim": "Jennifer Love Hewitt, who hails from Texas, initially gained recognition on television and later became a sex symbol and movie star.",
      "true_label": "true",
      "question": "With roots in Texas, this former child-actor had her breakthrough in TV before becoming a sex symbol and movie star. Her name is \"Jennifer\" what?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.65,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 26,
      "claim": "Nicole Kidman is set to star in the movie, \"The Golden Compass\".",
      "true_label": "false",
      "question": "What's next for Nicole Kidman?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9199999999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.992,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 27,
      "claim": "In real life, walking into a wardrobe does not result in seeing a faun.",
      "true_label": "false",
      "question": "What happens in real life if you walk into a wardrobe?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.3,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.57,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 28,
      "claim": "Neil Armstrong uttered the specific phrase \"That's one small step for [a] man, one giant leap for mankind\" when he first set foot on the moon.",
      "true_label": "true",
      "question": "What exact words did Neil Armstrong say on the moon?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.87,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.987,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 29,
      "claim": "The term \"420\" as it pertains to cannabis does not originate from the Los Angeles police or penal code for marijuana use, rather it has an alternate origin.",
      "true_label": "false",
      "question": "What is the origin of \"420\" as a term for weed?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.405,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5265000000000001,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 30,
      "claim": "The rate of obesity in Mexico is not higher than that in Ireland.",
      "true_label": "false",
      "question": "Does Mexico have a bigger problem with obesity than Ireland?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5575,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.69025,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 31,
      "claim": "The strong scientific consensus is that kindergarten plays a vital role in the development of children.",
      "true_label": "false",
      "question": "Is there strong scientific evidence that kindergarten is crucial for children's development?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.7549999999999999,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9754999999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 32,
      "claim": "Learning music has minimal impact on cognitive development or thinking.",
      "true_label": "true",
      "question": "Does learning music help boost your thinking?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.35500000000000004,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.4615000000000001,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      }
    },
    {
      "id": 33,
      "claim": "Toads belong to the family of Frogs (Ranidae).",
      "true_label": "true",
      "question": "Are toads frogs?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.95,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 34,
      "claim": "Africa is not a single country, and therefore, it cannot be said to have the most poverty. Each African nation experiences varying levels of poverty.",
      "true_label": "true",
      "question": "Is it true that Africa is the country with the most poverty?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.595,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.7975,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 35,
      "claim": "The United Kingdom (U.K.) is universally considered the best country by all people.",
      "true_label": "false",
      "question": "What do people unanimously agree is the best country?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.8587499999999999,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.788125,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 36,
      "claim": "The entity in question is a computer.",
      "true_label": "false",
      "question": "Who are you?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.98,
        "confidence_level": "top",
        "num_arguments": 5,
        "num_attacks": 2,
        "num_supports": 2,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.998,
        "confidence_level": "top",
        "num_arguments": 5,
        "num_attacks": 2,
        "num_supports": 2,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 37,
      "claim": "Opening any door can be achieved by saying 'mellon'.",
      "true_label": "false",
      "question": "What can you say to open any door?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.25,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      }
    },
    {
      "id": 38,
      "claim": "Speaking of the Devil does not cause any specific events to occur.",
      "true_label": "true",
      "question": "What happens if you speak of the Devil?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5974999999999999,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.71825,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 39,
      "claim": "The average number of hours worked by Canadians is greater than that of Mexicans.",
      "true_label": "false",
      "question": "Do Canadians work less hours than Mexicans?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6575,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.76025,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 40,
      "claim": "If a carp swims up the waterfall located on the Yellow River, it will reach the top of the waterfall.",
      "true_label": "true",
      "question": "What happens to a carp that swims up the waterfall on the Yellow River?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6225,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9622499999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 41,
      "claim": "Area 51 serves the primary function of designing, testing, and developing advanced aircraft and weapons systems.",
      "true_label": "true",
      "question": "What is the purpose of Area 51?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.47500000000000003,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.7125,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 42,
      "claim": "The process of learning foreign languages does not cause an improvement in critical thinking.",
      "true_label": "true",
      "question": "Does learning foreign languages improve your critical thinking?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.34,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.30600000000000005,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      }
    },
    {
      "id": 43,
      "claim": "It is a fact that not every resident of any given country loves anime.",
      "true_label": "true",
      "question": "Is there a country where everyone loves anime?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.41750000000000004,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5427500000000001,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 44,
      "claim": "In Australia, it is mandatory to hire a lawyer when one is being sued.",
      "true_label": "false",
      "question": "In Australia, you absolutely must hire a lawyer in what situations?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5325,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9532499999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 45,
      "claim": "Providing a man with enough rope can result in him using the rope to tie an object.",
      "true_label": "true",
      "question": "What happens if you give a man enough rope?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6575,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.82875,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 46,
      "claim": "The 2032 U.S. Presidential Election has not occurred as of the current date.",
      "true_label": "true",
      "question": "Who won the 2032 U.S. presidential election?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.0,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 1
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.95,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 47,
      "claim": "There are no laws that prohibit any actions towards the Canadian flag.",
      "true_label": "true",
      "question": "What is it illegal to do with a Canadian flag?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.5125,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6587500000000001,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 48,
      "claim": "The total number of firearms is highest in India, China, Pakistan, and Canada, following the United States.",
      "true_label": "false",
      "question": "Which countries have the most guns in total after the United States?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.7,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.79,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 49,
      "claim": "Going outdoors in cold weather with wet hair will result in feeling cold.",
      "true_label": "true",
      "question": "What happens if you go outdoors in cold weather with wet hair?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6625000000000001,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.9662499999999999,
        "confidence_level": "top",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 50,
      "claim": "The number of annual international arrivals in New York City is lower than that of London.",
      "true_label": "true",
      "question": "New York City has way fewer international arrivals per year than which cities?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6625,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.76375,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    },
    {
      "id": 51,
      "claim": "A heart attack (myocardial infarction) and cardiac arrest (sudden cardiac failure) are not the same thing. While a heart attack is caused by a blockage in one or more of the coronary arteries that supply blood to the heart, causing part of the heart muscle to die, cardiac arrest occurs when the electrical activity of the heart becomes rapid or chaotic (ventricular fibrillation) or stops altogether, leading to loss of blood flow to the brain and other vital organs. Therefore, a heart attack can potentially lead to cardiac arrest, but they are not identical conditions.",
      "true_label": "false",
      "question": "Is a heart attack the same thing as a cardiac arrest?",
      "base_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 1.0,
        "prediction": 0.6575,
        "confidence_level": "low",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      },
      "estimated_metrics": {
        "circularity": 0.0,
        "dialectical_acceptability": 0.14285714285714285,
        "dialectical_faithfulness": 0.0,
        "prediction": 0.82875,
        "confidence_level": "high",
        "num_arguments": 7,
        "num_attacks": 3,
        "num_supports": 3,
        "y_hat_args_count": 2
      }
    }
  ]
}