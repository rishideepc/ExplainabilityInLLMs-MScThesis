{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e306529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import metric evaluation from your existing codebase\n",
    "from evaluation.explanation_evaluation_calc import evaluate_all_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2f5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path structure\n",
    "datasets = [\"truthfulqa\", \"strategyqa\", \"medqa\", \"commonsenseqa\"]\n",
    "models = [\"mistral\", \"llama\", \"qwen\"]\n",
    "metric_keys = [\"redundancy\", \"weak_relevance\", \"strong_relevance\"]\n",
    "base_path = os.path.join(project_root, \"results\", \"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec7e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\truthfulqa_mistral\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\truthfulqa_llama\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\truthfulqa_qwen\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\strategyqa_mistral\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\strategyqa_llama\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\strategyqa_qwen\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\medqa_mistral\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\medqa_llama\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\medqa_qwen\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\commonsenseqa_mistral\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\commonsenseqa_llama\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n",
      "[✓] Evaluating: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\generation\\commonsenseqa_qwen\\cot_outputs_ollama_meta_reasoning_conclusion_step_indices_fewshot.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Create master dictionary to collect average scores\n",
    "summary = defaultdict(dict)\n",
    "\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        subfolder = f\"{dataset}_{model}\"\n",
    "        folder_path = os.path.join(base_path, subfolder)\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"[SKIPPED] Missing folder: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        jsonl_files = [f for f in os.listdir(folder_path) if f.endswith(\".jsonl\")]\n",
    "        if not jsonl_files:\n",
    "            print(f\"[SKIPPED] No .jsonl file in {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(folder_path, jsonl_files[0])\n",
    "        print(f\"[✓] Evaluating: {filepath}\")\n",
    "\n",
    "        try:\n",
    "            results = evaluate_all_cot(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to evaluate: {filepath}\")\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        for metric in metric_keys:\n",
    "            try:\n",
    "                avg_score = sum(entry[metric] for entry in results) / len(results)\n",
    "                summary[dataset][f\"{model}_{metric}\"] = round(avg_score, 4)\n",
    "            except Exception as e:\n",
    "                summary[dataset][f\"{model}_{metric}\"] = \"N/A\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec367b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert summary into DataFrame\n",
    "df = pd.DataFrame.from_dict(summary, orient=\"index\").reset_index()\n",
    "df.rename(columns={\"index\": \"Dataset\"}, inplace=True)\n",
    "\n",
    "# Capitalize dataset names\n",
    "dataset_name_map = {\n",
    "    \"truthfulqa\": \"TruthfulQA\",\n",
    "    \"strategyqa\": \"StrategyQA\",\n",
    "    \"medqa\": \"MedQA\",\n",
    "    \"commonsenseqa\": \"CommonSenseQA\"\n",
    "}\n",
    "df[\"Dataset\"] = df[\"Dataset\"].map(dataset_name_map)\n",
    "\n",
    "# Extract column data and build MultiIndex columns\n",
    "column_tuples = []\n",
    "new_data = {}\n",
    "\n",
    "for model in models:\n",
    "    pretty_model = model.capitalize() if model != \"llama\" else \"LLaMA\"\n",
    "    for metric in metric_keys:\n",
    "        pretty_metric = metric.replace(\"_\", \" \").title()\n",
    "        flat_col = f\"{model}_{metric}\"\n",
    "        multi_col = (pretty_model, pretty_metric)\n",
    "        column_tuples.append(multi_col)\n",
    "        new_data[multi_col] = df[flat_col]\n",
    "\n",
    "# Create MultiIndex DataFrame\n",
    "multi_df = pd.DataFrame(new_data)\n",
    "multi_df.insert(0, (\"\", \"Dataset\"), df[\"Dataset\"])  # Insert Dataset as top-level blank\n",
    "multi_df.columns = pd.MultiIndex.from_tuples(multi_df.columns)\n",
    "\n",
    "# Overwrite df with multi_df\n",
    "df = multi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbdf466d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "th.col_heading.level0 {\n",
       "    text-align: center !important;\n",
       "    padding: 10px 12px;\n",
       "    background-color: #f9f9f9;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       "th.col_heading.level1 {\n",
       "    text-align: center !important;\n",
       "    padding: 8px 16px; /* Increased left-right padding */\n",
       "}\n",
       "\n",
       "td {\n",
       "    text-align: center;\n",
       "    padding: 6px 10px;\n",
       "}\n",
       "</style>\n",
       "<div style=\"max-height: 500px; overflow: auto;\">\n",
       "<table class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Mistral</th>\n",
       "      <th colspan=\"3\" halign=\"left\">LLaMA</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Qwen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th>Redundancy</th>\n",
       "      <th>Weak Relevance</th>\n",
       "      <th>Strong Relevance</th>\n",
       "      <th>Redundancy</th>\n",
       "      <th>Weak Relevance</th>\n",
       "      <th>Strong Relevance</th>\n",
       "      <th>Redundancy</th>\n",
       "      <th>Weak Relevance</th>\n",
       "      <th>Strong Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TruthfulQA</td>\n",
       "      <td>0.1989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>StrategyQA</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2728</td>\n",
       "      <td>0.7478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MedQA</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.2485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CommonSenseQA</td>\n",
       "      <td>0.1956</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.1536</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2749</td>\n",
       "      <td>0.7461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display styled multi-index table with centered, padded subheaders\n",
    "custom_table_html = f\"\"\"\n",
    "<style>\n",
    "th.col_heading.level0 {{\n",
    "    text-align: center !important;\n",
    "    padding: 10px 12px;\n",
    "    background-color: #f9f9f9;\n",
    "    font-weight: bold;\n",
    "}}\n",
    "\n",
    "th.col_heading.level1 {{\n",
    "    text-align: center !important;\n",
    "    padding: 8px 16px; /* Increased left-right padding */\n",
    "}}\n",
    "\n",
    "td {{\n",
    "    text-align: center;\n",
    "    padding: 6px 10px;\n",
    "}}\n",
    "</style>\n",
    "<div style=\"max-height: 500px; overflow: auto;\">\n",
    "{df.to_html(index=False, escape=False, border=0)}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(custom_table_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57de562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] CSV saved to: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\cot_method\\cot_metrics_summary.csv\n",
      "[✓] HTML saved to: c:\\Users\\rishi\\Desktop\\ImperialMSc\\Individual_projec\\Code\\ExplainabilityInLLMs-MScThesis\\results\\cot_method\\cot_metrics_summary.html\n"
     ]
    }
   ],
   "source": [
    "output_csv = os.path.join(project_root, \"results\", \"cot_method\", \"cot_metrics_summary.csv\")\n",
    "output_html = os.path.join(project_root, \"results\", \"cot_method\", \"cot_metrics_summary.html\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"[✓] CSV saved to: {output_csv}\")\n",
    "\n",
    "# Save as HTML\n",
    "df.to_html(output_html, index=False)\n",
    "print(f\"[✓] HTML saved to: {output_html}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
